import warnings
warnings.filterwarnings('ignore')
import torch
import torch.nn as nn
import torch.onnx
from torchinfo import summary
from torchviz import make_dot
import time


# ==========================================================================================
# è‡ªå®šä¹‰ FLOPs è®¡ç®— Hooks (ç”¨äº ptflops ä¸é»˜è®¤æ”¯æŒçš„æ“ä½œ)
# ==========================================================================================

def gru_flops_counter_hook(module, input, output):
    """
    GRU çš„ FLOPs è®¡ç®— Hook
    
    GRU æ¯ä¸ªæ—¶é—´æ­¥çš„è®¡ç®—é‡:
    - 3ä¸ªé—¨ (reset, update, new): æ¯ä¸ªé—¨éœ€è¦ 2 * (input_size + hidden_size) * hidden_size æ¬¡ä¹˜åŠ 
    - æ€»è®¡: 6 * (input_size + hidden_size) * hidden_size æ¬¡ä¹˜åŠ  (MACs)
    - åŒå‘: å†ä¹˜ä»¥ 2
    - å¤šå±‚: ç¬¬ä¸€å±‚ input_size ä¸ºåŸå§‹è¾“å…¥ï¼Œåç»­å±‚ä¸º hidden_size * num_directions
    
    æ³¨æ„: 1 MAC = 2 FLOPs (ä¸€æ¬¡ä¹˜æ³• + ä¸€æ¬¡åŠ æ³•)
    """
    input_tensor = input[0]
    batch_size = input_tensor.size(0)
    seq_len = input_tensor.size(1)
    input_size = module.input_size
    hidden_size = module.hidden_size
    num_layers = module.num_layers
    bidirectional = module.bidirectional
    num_directions = 2 if bidirectional else 1
    
    total_macs = 0
    
    for layer in range(num_layers):
        # ç¬¬ä¸€å±‚ä½¿ç”¨åŸå§‹ input_sizeï¼Œåç»­å±‚ä½¿ç”¨ hidden_size * num_directions
        layer_input_size = input_size if layer == 0 else hidden_size * num_directions
        
        # æ¯ä¸ªæ—¶é—´æ­¥çš„ MACs (3ä¸ªé—¨ï¼Œæ¯ä¸ªé—¨æœ‰ä¸¤ä¸ªçŸ©é˜µä¹˜æ³•)
        macs_per_step = 3 * 2 * (layer_input_size + hidden_size) * hidden_size
        
        # åºåˆ—é•¿åº¦ * æ‰¹æ¬¡å¤§å°
        layer_macs = macs_per_step * seq_len * batch_size
        
        # åŒå‘åˆ™ç¿»å€
        layer_macs *= num_directions
        
        total_macs += layer_macs
    
    # è®¾ç½®è®¡ç®—é‡ (ptflops ä½¿ç”¨ __flops__ å±æ€§)
    module.__flops__ += int(total_macs)


def pixelshuffle1d_flops_counter_hook(module, input, output):
    """
    PixelShuffle1D çš„ FLOPs è®¡ç®— Hook
    
    PixelShuffle ä»…è¿›è¡Œå¼ é‡é‡æ’ï¼Œç†è®ºä¸Šæ²¡æœ‰æµ®ç‚¹è¿ç®—
    ä½†åœ¨å®é™…å®ç°ä¸­å¯èƒ½æœ‰ä¸€äº› view/permute æ“ä½œçš„å¼€é”€ï¼Œè¿™é‡Œè®¾ä¸º 0
    """
    module.__flops__ += 0


def calculate_flops_with_ptflops(model, input_data, device, print_per_layer=False):
    """
    ä½¿ç”¨ ptflops è®¡ç®—æ¨¡å‹çš„ FLOPs
    
    å‚æ•°:
        model: PyTorch æ¨¡å‹
        input_data: è¾“å…¥æ•°æ® (tuple æˆ– tensor)
        device: è®¡ç®—è®¾å¤‡
        print_per_layer: æ˜¯å¦æ‰“å°æ¯å±‚çš„ FLOPs è¯¦æƒ…
    
    è¿”å›:
        dict: åŒ…å« FLOPsã€MACsã€å‚æ•°é‡ç­‰ä¿¡æ¯çš„å­—å…¸
    """
    from ptflops import get_model_complexity_info
    from ptflops.pytorch_ops import MODULES_MAPPING
    
    # è·å–è¾“å…¥å½¢çŠ¶ (ä¸åŒ…å« batch ç»´åº¦)
    if isinstance(input_data, (tuple, list)):
        input_shape = tuple(input_data[0].shape[1:])  # ç§»é™¤ batch ç»´åº¦
    else:
        input_shape = tuple(input_data.shape[1:])
    
    # åˆ›å»ºè‡ªå®šä¹‰ hooks å­—å…¸
    # å¯¼å…¥æ¨¡å‹æ¨¡å—ä»¥è·å–è‡ªå®šä¹‰ç±»
    try:
        from model.model import PixelShuffle1D, BiGRUBottleneck
        custom_hooks = {
            nn.GRU: gru_flops_counter_hook,
            PixelShuffle1D: pixelshuffle1d_flops_counter_hook,
        }
    except ImportError:
        custom_hooks = {
            nn.GRU: gru_flops_counter_hook,
        }
    
    # å®šä¹‰è¾“å…¥æ„é€ å‡½æ•°
    def input_constructor(input_res):
        return torch.randn(1, *input_res).to(device)
    
    model.eval()
    
    try:
        macs, params = get_model_complexity_info(
            model, 
            input_shape,
            input_constructor=input_constructor,
            as_strings=False,
            print_per_layer_stat=print_per_layer,
            verbose=print_per_layer,
            custom_modules_hooks=custom_hooks,
        )
        
        # MACs to FLOPs: 1 MAC â‰ˆ 2 FLOPs
        flops = macs * 2
        
        return {
            'macs': macs,
            'flops': flops,
            'params': params,
            'success': True
        }
        
    except Exception as e:
        print(f"   âš ï¸  ptflops è®¡ç®—å¤±è´¥: {e}")
        return {
            'macs': 0,
            'flops': 0,
            'params': sum(p.numel() for p in model.parameters()),
            'success': False,
            'error': str(e)
        }


def print_flops_analysis(flops_info, batch_size=1):
    """
    æ‰“å° FLOPs åˆ†æç»“æœå’ŒæŒ‡æ ‡è§£é‡Š
    
    å‚æ•°:
        flops_info: calculate_flops_with_ptflops è¿”å›çš„å­—å…¸
        batch_size: æ‰¹æ¬¡å¤§å°
    """
    macs = flops_info['macs']
    flops = flops_info['flops']
    params = flops_info['params']
    
    print("\n   " + "â”€" * 60)
    print("   ğŸ“Š æ ¸å¿ƒæŒ‡æ ‡")
    print("   " + "â”€" * 60)
    
    # å‚æ•°é‡
    print(f"   å‚æ•°é‡ (Params)      : {params:>15,} ({params/1e6:.2f} M)")
    
    # MACs
    print(f"   ä¹˜åŠ æ¬¡æ•° (MACs)      : {macs:>15,} ({macs/1e9:.3f} G)")
    
    # FLOPs
    print(f"   æµ®ç‚¹è¿ç®—æ•° (FLOPs)   : {flops:>15,} ({flops/1e9:.3f} G)")
    
    # æ¢ç®—ä¸º TOPS (Tera Operations Per Second)
    print("\n   " + "â”€" * 60)
    print("   âš¡ ç®—åŠ›éœ€æ±‚ (å‡è®¾æ»¡è´Ÿè½½æ¨ç†)")
    print("   " + "â”€" * 60)
    
    fps_targets = [30, 60, 120, 1000]
    for fps in fps_targets:
        tops = (flops * fps) / 1e12
        gflops = (flops * fps) / 1e9
        label = f"@{fps}fps (batch={batch_size})"
        print(f"   {label:<25}: {gflops:>8.2f} GFLOPS = {tops:.4f} TOPS")
    
    # æ·»åŠ å…¸å‹ç¡¬ä»¶å‚è€ƒ
    print("\n   " + "â”€" * 60)
    print("   ğŸ–¥ï¸  å…¸å‹ç¡¬ä»¶ç®—åŠ›å‚è€ƒ (FP32)")
    print("   " + "â”€" * 60)
    print("   RTX 3060              :      12.74 TFLOPS")
    print("   RTX 3080              :      29.77 TFLOPS")
    print("   RTX 4090              :      82.58 TFLOPS")
    print("   A100 (40GB)           :      19.49 TFLOPS")
    print("   åµŒå…¥å¼ Jetson Orin    :       5.32 TFLOPS")
    print("   è½¦è§„çº§ TDA4VM         :       8.00 TOPS (INT8)")
    
    # æŒ‡æ ‡è§£é‡Š
    print("\n   " + "â”€" * 60)
    print("   ğŸ“– æŒ‡æ ‡è§£é‡Š")
    print("   " + "â”€" * 60)
    print("""
   â€¢ å‚æ•°é‡ (Parameters)
     æ¨¡å‹ä¸­å¯å­¦ä¹ æƒé‡çš„æ€»æ•°ã€‚å½±å“æ¨¡å‹å­˜å‚¨å¤§å°å’Œå†…å­˜å ç”¨ã€‚
     1M å‚æ•° â‰ˆ 4MB (FP32) / 2MB (FP16) / 1MB (INT8)

   â€¢ MACs (Multiply-Accumulate Operations)
     ä¹˜åŠ è¿ç®—æ¬¡æ•°ã€‚ä¸€æ¬¡ MAC = ä¸€æ¬¡ä¹˜æ³• + ä¸€æ¬¡åŠ æ³•ã€‚
     è¿™æ˜¯æ·±åº¦å­¦ä¹ ä¸­æœ€æ ¸å¿ƒçš„è®¡ç®—å•å…ƒã€‚

   â€¢ FLOPs (Floating Point Operations)
     æµ®ç‚¹è¿ç®—æ¬¡æ•°ã€‚é€šå¸¸ FLOPs â‰ˆ 2 Ã— MACsã€‚
     æ³¨æ„: ä¸åŒæ–‡çŒ®å¯èƒ½æ··ç”¨ FLOPs å’Œ MACsï¼Œéœ€æ³¨æ„åŒºåˆ†ã€‚

   â€¢ GFLOPS / TFLOPS (Giga/Tera FLOPs Per Second)
     æ¯ç§’åäº¿/ä¸‡äº¿æ¬¡æµ®ç‚¹è¿ç®—ï¼Œè¡¡é‡ç®—åŠ›éœ€æ±‚æˆ–ç¡¬ä»¶æ€§èƒ½ã€‚
     æ¨¡å‹éœ€æ±‚: FLOPs Ã— FPS; ç¡¬ä»¶ä¾›ç»™: å³°å€¼ TFLOPS

   â€¢ TOPS (Tera Operations Per Second)
     æ¯ç§’ä¸‡äº¿æ¬¡è¿ç®— (é€šå¸¸æŒ‡ INT8/INT4 æ•´æ•°è¿ç®—)ã€‚
     1 TOPS = 1e12 OPSã€‚å¸¸ç”¨äºè¡¡é‡ NPU/VPU æ€§èƒ½ã€‚

   â€¢ ç®—åŠ›åˆ©ç”¨ç‡
     å®é™…æ¨ç†æ—¶ï¼Œç”±äºå†…å­˜å¸¦å®½ã€å¹¶è¡Œæ•ˆç‡ç­‰å› ç´ ï¼Œ
     é€šå¸¸åªèƒ½è¾¾åˆ°ç¡¬ä»¶å³°å€¼ç®—åŠ›çš„ 30%-70%ã€‚
""")


def test_model(
    model,
    inputs,
    labels,
    criterion=None,
    optimizer=None,
    onnx_file_path="model_test.onnx",
    test_inference_speed=True,
    num_warmup=10,
    num_iterations=100,
    print_flops_per_layer=False  # æ–°å¢å‚æ•°ï¼šæ˜¯å¦æ‰“å°æ¯å±‚ FLOPs
):
    """
    é€šç”¨åŒ–æ¨¡å‹æµ‹è¯•å‡½æ•°ï¼š
    1. æ¥å—ä»»æ„æ¨¡å‹å®ä¾‹åŒ–å¯¹è±¡ `model`ã€‚
    2. è‡ªå®šä¹‰è¾“å…¥ `inputs` å’Œæ ‡ç­¾ `labels`ã€‚
    3. æ”¯æŒå‰å‘ä¼ æ’­ã€åå‘ä¼ æ’­ã€æŸå¤±è®¡ç®—ã€‚
    4. å¯¼å‡º ONNX æ¨¡å‹å¹¶éªŒè¯ã€‚
    5. è¾“å‡ºæ¨¡å‹è¯¦ç»†ä¿¡æ¯ã€‚
    6. è®¡ç®— FLOPS å’Œæ¨ç†é€Ÿåº¦ï¼ˆç”¨äºè¯„ä¼°ç®—åŠ›éœ€æ±‚ï¼‰ã€‚
    
    å‚æ•°ï¼š
    - model: PyTorch æ¨¡å‹å®ä¾‹åŒ–å¯¹è±¡
    - inputs: æ¨¡å‹çš„è¾“å…¥å¼ é‡ (tensor / tuple / list)
    - labels: æ¨¡å‹çš„çœŸå®æ ‡ç­¾å¼ é‡ï¼ˆç”¨äºæŸå¤±è®¡ç®—ï¼‰
    - criterion: æŸå¤±å‡½æ•°å®ä¾‹åŒ–å¯¹è±¡ï¼Œé»˜è®¤ä¸º nn.MSELoss
    - optimizer: ä¼˜åŒ–å™¨å®ä¾‹åŒ–å¯¹è±¡ï¼Œé»˜è®¤ä¸º Adam
    - onnx_file_path: å¯¼å‡ºçš„ ONNX æ–‡ä»¶è·¯å¾„
    - test_inference_speed: æ˜¯å¦æµ‹è¯•æ¨ç†é€Ÿåº¦ï¼Œé»˜è®¤ True
    - num_warmup: é¢„çƒ­æ¬¡æ•°ï¼Œé»˜è®¤ 10
    - num_iterations: æµ‹è¯•æ¨ç†æ¬¡æ•°ï¼Œé»˜è®¤ 100
    - print_flops_per_layer: æ˜¯å¦æ‰“å°æ¯å±‚çš„ FLOPs è¯¦æƒ…ï¼Œé»˜è®¤ False
    """
    # ==================== åˆå§‹åŒ–è®¾ç½® ====================
    if criterion is None:
        criterion = nn.MSELoss()
    if optimizer is None:
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    # ç»Ÿä¸€å¤„ç†è¾“å…¥æ•°æ®æ ¼å¼
    _input_data = tuple(inputs) if isinstance(inputs, (tuple, list)) else inputs
    batch_size = inputs[0].shape[0] if isinstance(inputs, (tuple, list)) else inputs.shape[0]
    device = next(model.parameters()).device
    original_training = model.training
    
    # batch_size=1 æ—¶å¿…é¡»ä½¿ç”¨ eval æ¨¡å¼ï¼ˆé¿å… BatchNorm é”™è¯¯ï¼‰
    use_eval_mode = (batch_size == 1)
    
    print("\n" + "=" * 80)
    print("ğŸš€ å¼€å§‹æµ‹è¯•ç¥ç»ç½‘ç»œæ¨¡å‹")
    print("=" * 80)
    
    if use_eval_mode:
        print(f"\nâš ï¸  batch_size=1ï¼Œå…¨ç¨‹ä½¿ç”¨è¯„ä¼°æ¨¡å¼ (model.eval())")
        print("   æç¤ºï¼šå¦‚éœ€æµ‹è¯•åå‘ä¼ æ’­ï¼Œè¯·å°† batch_size è®¾ç½®ä¸º > 1")
    else:
        print(f"\nâœ… batch_size={batch_size}ï¼Œæ”¯æŒè®­ç»ƒæ¨¡å¼æµ‹è¯•")
    
    # ==================== æ¨¡å‹ç»“æ„ä¿¡æ¯ ====================
    print("\n" + "-" * 40)
    print("ğŸ“Š æ¨¡å‹ç»“æ„ä¿¡æ¯")
    print("-" * 40)
    
    try:
        model.eval()
        summary(
            model,
            input_data=_input_data,
            col_names=["input_size", "output_size", "num_params"],
            depth=3,
            device=str(device)
        )
    except Exception as e:
        print(f"âš ï¸  torchinfo.summary æ‰§è¡Œå¤±è´¥: {e}")
    
    # ==================== ç®—åŠ›éœ€æ±‚è¯„ä¼° (ä½¿ç”¨ ptflops) ====================
    print("\n" + "-" * 40)
    print("âš¡ ç®—åŠ›éœ€æ±‚è¯„ä¼° (ä½¿ç”¨ ptflops)")
    print("-" * 40)
    
    flops_info = calculate_flops_with_ptflops(
        model, 
        _input_data, 
        device,
        print_per_layer=print_flops_per_layer
    )
    
    if flops_info['success']:
        print_flops_analysis(flops_info, batch_size=batch_size)
    else:
        print(f"   âš ï¸  FLOPs ç»Ÿè®¡å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ fvcore å¤‡ç”¨æ–¹æ¡ˆ...")
        try:
            from fvcore.nn import FlopCountAnalysis, parameter_count
            
            model.eval()
            flops_analysis = FlopCountAnalysis(model, _input_data)
            total_flops = flops_analysis.total()
            params = parameter_count(model)['']
            
            backup_info = {
                'macs': total_flops // 2,  # è¿‘ä¼¼è½¬æ¢
                'flops': total_flops,
                'params': params,
                'success': True
            }
            print_flops_analysis(backup_info, batch_size=batch_size)
        except Exception as e:
            print(f"   âš ï¸  å¤‡ç”¨æ–¹æ¡ˆä¹Ÿå¤±è´¥: {e}")
            print(f"   å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # ==================== æ¨ç†é€Ÿåº¦æµ‹è¯• ====================
    if test_inference_speed:
        print("\n" + "-" * 40)
        print("â±ï¸  æ¨ç†é€Ÿåº¦æµ‹è¯•")
        print("-" * 40)
        
        model.eval()
        with torch.no_grad():
            # é¢„çƒ­
            for _ in range(num_warmup):
                _ = model(*_input_data) if isinstance(_input_data, tuple) else model(_input_data)
            
            if device.type == 'cuda':
                torch.cuda.synchronize()
            
            # è®¡æ—¶æµ‹è¯•
            start_time = time.time()
            for _ in range(num_iterations):
                _ = model(*_input_data) if isinstance(_input_data, tuple) else model(_input_data)
            
            if device.type == 'cuda':
                torch.cuda.synchronize()
            end_time = time.time()
            
            avg_time = (end_time - start_time) / num_iterations * 1000
            fps = 1000 / avg_time
            
            print(f"   é¢„çƒ­æ¬¡æ•°    : {num_warmup}")
            print(f"   æµ‹è¯•æ¬¡æ•°    : {num_iterations}")
            print(f"   å¹³å‡è€—æ—¶    : {avg_time:.2f} ms")
            print(f"   æ¨ç†é€Ÿåº¦    : {fps:.2f} FPS")
            print(f"   ååé‡      : {fps * batch_size:.2f} samples/s")
            
            # è®¡ç®—å®é™…ç®—åŠ›æ¶ˆè€—
            if flops_info['success']:
                actual_gflops = (flops_info['flops'] * fps) / 1e9
                actual_tops = actual_gflops / 1000
                print(f"   å®é™…ç®—åŠ›æ¶ˆè€—: {actual_gflops:.2f} GFLOPS ({actual_tops:.4f} TOPS)")
            
            if device.type == 'cuda':
                memory = torch.cuda.max_memory_allocated(device) / 1024**2
                print(f"   GPUå†…å­˜å ç”¨ : {memory:.2f} MB")
                torch.cuda.reset_peak_memory_stats(device)
    
    # ==================== å‰å‘ä¼ æ’­ ====================
    print("\n" + "-" * 40)
    print("ğŸ”„ å‰å‘ä¼ æ’­æµ‹è¯•")
    print("-" * 40)
    
    # æ ¹æ® batch_size è®¾ç½®æ¨¡å¼
    if use_eval_mode:
        model.eval()
    else:
        model.train()
    
    outputs = model(*_input_data) if isinstance(_input_data, tuple) else model(_input_data)
    
    # æ‰“å°è¾“å…¥ä¿¡æ¯
    if isinstance(inputs, (tuple, list)):
        input_shapes = [str(tuple(inp.shape)) for inp in inputs]
        print(f"   è¾“å…¥å½¢çŠ¶    : [{', '.join(input_shapes)}]")
    else:
        print(f"   è¾“å…¥å½¢çŠ¶    : {tuple(inputs.shape)}")
    
    # æ‰“å°è¾“å‡ºä¿¡æ¯
    if isinstance(outputs, (tuple, list)) and not isinstance(outputs, torch.Tensor):
        output_info = _format_output_structure(outputs)
        print(f"   è¾“å‡ºç»“æ„    : [{', '.join(output_info)}]")
        loss, matched_output = _compute_loss_multi_output(outputs, labels, criterion)
    else:
        print(f"   è¾“å‡ºå½¢çŠ¶    : {tuple(outputs.shape)}")
        loss, matched_output = _compute_loss_single_output(outputs, labels, criterion)
    
    if loss is not None:
        print(f"   æŸå¤±å€¼      : {loss.item():.6f}")
    
    # ==================== åå‘ä¼ æ’­ ====================
    print("\n" + "-" * 40)
    print("ğŸ”™ åå‘ä¼ æ’­æµ‹è¯•")
    print("-" * 40)
    
    if use_eval_mode:
        print("   â­ï¸  è·³è¿‡ï¼ˆbatch_size=1 ä¸æ”¯æŒè®­ç»ƒæ¨¡å¼ï¼‰")
    elif loss is None:
        print("   â­ï¸  è·³è¿‡ï¼ˆæ— æœ‰æ•ˆæŸå¤±å€¼ï¼‰")
    else:
        try:
            model.train()
            optimizer.zero_grad()
            
            # éœ€è¦é‡æ–°å‰å‘ä¼ æ’­ï¼ˆå› ä¸ºä¹‹å‰çš„è®¡ç®—å›¾å¯èƒ½å·²æ–­å¼€ï¼‰
            outputs_train = model(*_input_data) if isinstance(_input_data, tuple) else model(_input_data)
            if isinstance(outputs_train, (tuple, list)) and not isinstance(outputs_train, torch.Tensor):
                loss_train, _ = _compute_loss_multi_output(outputs_train, labels, criterion)
            else:
                loss_train, _ = _compute_loss_single_output(outputs_train, labels, criterion)
            
            loss_train.backward()
            optimizer.step()
            print("   âœ… åå‘ä¼ æ’­æ­£å¸¸")
        except Exception as e:
            print(f"   âŒ åå‘ä¼ æ’­å¤±è´¥: {e}")
    
    # ==================== è®¡ç®—å›¾å¯è§†åŒ– ====================
    print("\n" + "-" * 40)
    print("ğŸ“ˆ è®¡ç®—å›¾å¯è§†åŒ–")
    print("-" * 40)
    
    try:
        if loss is not None:
            graph = make_dot(loss, params=dict(model.named_parameters()))
            graph.render("model_computation_graph", format="png", cleanup=True)
            print("   âœ… å·²ä¿å­˜: model_computation_graph.png")
        else:
            print("   â­ï¸  è·³è¿‡ï¼ˆæ— æœ‰æ•ˆæŸå¤±å€¼ï¼‰")
    except Exception as e:
        print(f"   âš ï¸  å¤±è´¥: {e}")
    
    # ==================== ONNX å¯¼å‡º ====================
    print("\n" + "-" * 40)
    print("ğŸ“¦ ONNX æ¨¡å‹å¯¼å‡º")
    print("-" * 40)
    
    try:
        model.eval()
        
        # é…ç½®è¾“å…¥è¾“å‡ºåç§°
        if isinstance(inputs, (tuple, list)):
            input_names = [f"input_{i}" for i in range(len(inputs))]
            dynamic_axes = {name: {0: "batch_size"} for name in input_names}
        else:
            input_names = ["input"]
            dynamic_axes = {"input": {0: "batch_size"}}
        
        output_names, output_dynamic = _get_output_names(outputs)
        dynamic_axes.update(output_dynamic)
        
        torch.onnx.export(
            model, _input_data, onnx_file_path,
            input_names=input_names,
            output_names=output_names,
            dynamic_axes=dynamic_axes,
            opset_version=11,
        )
        print(f"   âœ… å·²ä¿å­˜: {onnx_file_path}")
        print("   ğŸ“ å¯è§†åŒ–: https://netron.app/")
    except Exception as e:
        print(f"   âš ï¸  å¯¼å‡ºå¤±è´¥: {e}")
    
    # æ¢å¤åŸå§‹çŠ¶æ€
    model.train(original_training)
    
    print("\n" + "=" * 80)
    print("âœ… æ¨¡å‹æµ‹è¯•å®Œæˆ")
    print("=" * 80 + "\n")


def _format_output_structure(outputs):
    """æ ¼å¼åŒ–å¤šè¾“å‡ºç»“æ„ä¿¡æ¯"""
    info = []
    for output in outputs:
        if isinstance(output, torch.Tensor):
            info.append(f"Tensor{tuple(output.shape)}")
        elif isinstance(output, (tuple, list)):
            sub = [f"Tensor{tuple(o.shape)}" if isinstance(o, torch.Tensor) else str(type(o).__name__) for o in output]
            info.append(f"({', '.join(sub)})")
        else:
            info.append(type(output).__name__)
    return info


def _compute_loss_multi_output(outputs, labels, criterion):
    """ä»å¤šè¾“å‡ºä¸­è®¡ç®—æŸå¤±"""
    for output in outputs:
        current = output[0] if isinstance(output, (tuple, list)) else output
        if isinstance(current, torch.Tensor) and current.shape == labels.shape:
            return criterion(current, labels), current
    
    # ä½¿ç”¨æœ€åä¸€ä¸ªè¾“å‡º
    last = outputs[-1]
    last = last[0] if isinstance(last, (tuple, list)) else last
    if isinstance(last, torch.Tensor):
        if last.shape[-1] != labels.shape[-1]:
            labels = nn.functional.interpolate(labels, size=last.shape[-1], mode='linear', align_corners=False)
        return criterion(last, labels), last
    return None, None


def _compute_loss_single_output(outputs, labels, criterion):
    """ä»å•è¾“å‡ºè®¡ç®—æŸå¤±"""
    if outputs.shape == labels.shape:
        return criterion(outputs, labels), outputs
    if outputs.shape[-1] != labels.shape[-1]:
        labels = nn.functional.interpolate(labels, size=outputs.shape[-1], mode='linear', align_corners=False)
    return criterion(outputs, labels), outputs


def _get_output_names(outputs):
    """è·å–è¾“å‡ºåç§°å’ŒåŠ¨æ€è½´é…ç½®"""
    names, axes = [], {}
    idx = 0
    
    if isinstance(outputs, (tuple, list)) and not isinstance(outputs, torch.Tensor):
        for out in outputs:
            if isinstance(out, (tuple, list)):
                for sub in out:
                    if isinstance(sub, torch.Tensor):
                        names.append(f"output_{idx}")
                        axes[f"output_{idx}"] = {0: "batch_size"}
                        idx += 1
            elif isinstance(out, torch.Tensor):
                names.append(f"output_{idx}")
                axes[f"output_{idx}"] = {0: "batch_size"}
                idx += 1
    else:
        names = ["output"]
        axes = {"output": {0: "batch_size"}}
    
    return names, axes


if __name__ == "__main__":
    from utils import read_json
    from parse_config import ConfigParser
    import model.model as module_arch

    config = ConfigParser(read_json('config.json'))
    Pulsemodel = config.init_obj('arch', module_arch)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    Pulsemodel = Pulsemodel.to(device)

    # ä½¿ç”¨ batch_size > 1 ä»¥æ”¯æŒå®Œæ•´æµ‹è¯•ï¼ˆåŒ…æ‹¬åå‘ä¼ æ’­ï¼‰
    batch_size = 1
    x = torch.randn(batch_size, 3).to(device)
    y = torch.randn(batch_size, 3, 150).to(device)

    print(f"\n{'='*80}")
    print(f"æ¨¡å‹: {type(Pulsemodel).__name__} | è®¾å¤‡: {device}")
    print(f"è¾“å…¥: {tuple(x.shape)} | æ ‡ç­¾: {tuple(y.shape)}")
    print(f"{'='*80}")

    test_model(
        Pulsemodel, 
        inputs=x, 
        labels=y,
        print_flops_per_layer=False  # è®¾ä¸º True å¯æŸ¥çœ‹æ¯å±‚è¯¦æƒ…
    )
